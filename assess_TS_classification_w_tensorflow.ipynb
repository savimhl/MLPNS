{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/savimhl/MLPNS_MSavi/blob/main/assess_TS_classification_w_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Timeseries classification with a Transformer model\n",
        "\n",
        "**Author:** [Theodoros Ntakouris](https://github.com/ntakouris)<br>\n",
        "**Date created:** 2021/06/25<br>\n",
        "**Last modified:** 2021/08/05<br>\n",
        "**Description:** This notebook demonstrates how to do timeseries classification using a Transformer model."
      ],
      "metadata": {
        "id": "ZQ-nRNLhognE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS KERAS EXAMPLE OF APPLICATION OF TRANSFORMERS TO TIME SERIES ANALYSIS IS **WRONG**\n",
        "\n",
        "My student Willow Fox Fortino found that out... "
      ],
      "metadata": {
        "id": "X9L2K8IfPS5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This is the Transformer architecture from\n",
        "[Attention Is All You Need](https://arxiv.org/abs/1706.03762),\n",
        "applied to timeseries instead of natural language.\n",
        "\n",
        "This example requires TensorFlow 2.4 or higher.\n",
        "\n",
        "## Load the dataset\n",
        "\n",
        "We are going to use the same dataset and preprocessing as the\n",
        "[TimeSeries Classification from Scratch](https://keras.io/examples/timeseries/timeseries_classification_from_scratch)\n",
        "example."
      ],
      "metadata": {
        "id": "2-8RslF7ognH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf7pcMVatnVR",
        "outputId": "f209fec8-0b75-4bb4-ab24-ccd28b442c38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/MLPNS2023/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB7sWeyptvTZ",
        "outputId": "a9e7000c-7378-47bb-ba20-1f2ab3eac2a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MLPNS2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def readucr(filename):\n",
        "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
        "    y = data[:, 0]\n",
        "    x = data[:, 1:]\n",
        "    return x, y.astype(int)\n",
        "\n",
        "\n",
        "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
        "\n",
        "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
        "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
        "\n",
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]\n",
        "\n",
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0"
      ],
      "outputs": [],
      "metadata": {
        "id": "XMBJszHaognI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 1: \n",
        "\n",
        "the author provide no data exploration. That is not acceptable. Explore the data"
      ],
      "metadata": {
        "id": "Ddlb5cxOZ-xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pylab as pl\n"
      ],
      "metadata": {
        "id": "SKubiQOgb-8T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "0i3YHI21aut9",
        "outputId": "988317c5-7d33-456a-c35a-6842525828b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3601, 500, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "OkTXJv_MbbPt",
        "outputId": "8e272ba0-6bee-4a7b-9027-086abefa8bb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3601,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "9KhnTJmTbx3C",
        "outputId": "f82ed96d-11c5-4381-975e-6c3de2df4349",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1320, 500, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "eSkg-GBAbzNj",
        "outputId": "5c535aca-416c-40af-aabd-ae02d2669d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1320,)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[:,0,0].shape"
      ],
      "metadata": {
        "id": "qOUF-gRicuiU",
        "outputId": "be536fcb-8b71-4328-b397-bbc9fc6a37bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3601,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "13ZDleTSd4ex",
        "outputId": "6d0ffd53-e5bd-4950-ba9c-97c503d0aadf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pl.plot(x_train[:,0,0], y_train, \"o\")"
      ],
      "metadata": {
        "id": "1YuLwAzgb2d4",
        "outputId": "c25cf2cd-451c-4799-baca-704256d2e137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fafadc000d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0,0,:]"
      ],
      "metadata": {
        "id": "Xdik-J9Ne8YQ",
        "outputId": "f257c6ee-44c4-4a00-f1c5-79b639ca51cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.7318177])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.reshape(3601,500)"
      ],
      "metadata": {
        "id": "1YWRrb4chjB8",
        "outputId": "02903617-f9af-4dd6-9d7f-adee2ba5fb45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.7318177 , -1.9671547 , -2.0848232 , ..., -1.7841148 ,\n",
              "        -1.365738  , -0.87676011],\n",
              "       [ 0.32188338,  0.54127787,  0.65097511, ..., -0.01799078,\n",
              "         0.11292352,  0.32455893],\n",
              "       [ 1.6969573 ,  1.3223602 ,  0.79464377, ..., -0.43118226,\n",
              "        -0.25989818, -0.07695805],\n",
              "       ...,\n",
              "       [ 1.0447303 ,  1.2422232 ,  1.2054233 , ..., -1.1019314 ,\n",
              "        -0.15065363,  0.99321041],\n",
              "       [ 0.09851273,  0.26493666,  0.44534766, ..., -0.48315305,\n",
              "        -0.92026694, -1.2735211 ],\n",
              "       [-0.61422959, -0.87059136, -1.0892804 , ..., -0.42458124,\n",
              "        -0.26256821, -0.02022817]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Our model processes a tensor of shape `(batch size, sequence length, features)`,\n",
        "where `sequence length` is the number of time steps and `features` is each input\n",
        "timeseries.\n",
        "\n",
        "You can replace your classification RNN layers with this one: the\n",
        "inputs are fully compatible!"
      ],
      "metadata": {
        "id": "8mHX4SxaognJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "outputs": [],
      "metadata": {
        "id": "krGMC9mDognJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We include residual connections, layer normalization, and dropout.\n",
        "The resulting layer can be stacked multiple times.\n",
        "\n",
        "The projection layers are implemented through `keras.layers.Conv1D`."
      ],
      "metadata": {
        "id": "8vqf9PsfognJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "vZyK2X9PognJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main part of our model is now complete. We can stack multiple of those\n",
        "`transformer_encoder` blocks and we can also proceed to add the final\n",
        "Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n",
        "layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n",
        "our model down to a vector of features for each data point in the current\n",
        "batch. A common way to achieve this is to use a pooling layer. For\n",
        "this example, a `GlobalAveragePooling1D` layer is sufficient."
      ],
      "metadata": {
        "id": "rItYDH_LognK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "\n",
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "    return keras.Model(inputs, outputs)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "XvxrXqWKognK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate"
      ],
      "metadata": {
        "id": "wXT9pqcuognL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "\"\"\"\n",
        "create a model with :\n",
        "4 multiattention heads each size 256, \n",
        "4 transformer blocks\n",
        "4 neurons in the convolutional layers\n",
        "128 neurons in the feed forward layers\n",
        "dropout 40% on the transformer layers\n",
        "dropout 25% on the feed forward layers\n",
        "\n",
        "compile it with a sparse_categorical_crossentropy,\n",
        "choose the kearas adam optimizer with a learning rate of 1e-4\n",
        "monitor the sparse_categorical_accuracy metric\n",
        "\"\"\"\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "\n",
        "model = keras.models.load_model('transformer_h4_model.h5')\n",
        "model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, \n",
        "                                           restore_best_weights=True)]\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 500, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_32 (LayerN  (None, 500, 1)      2           ['input_5[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_16 (Multi  (None, 500, 1)      7169        ['layer_normalization_32[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_34 (Dropout)           (None, 500, 1)       0           ['multi_head_attention_16[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_31 (TFOpL  (None, 500, 1)      0           ['dropout_34[0][0]',             \n",
            " ambda)                                                           'input_5[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_33 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_31[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_30 (Conv1D)             (None, 500, 4)       8           ['layer_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_35 (Dropout)           (None, 500, 4)       0           ['conv1d_30[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_31 (Conv1D)             (None, 500, 1)       5           ['dropout_35[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_32 (TFOpL  (None, 500, 1)      0           ['conv1d_31[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_31[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_34 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_32[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_17 (Multi  (None, 500, 1)      7169        ['layer_normalization_34[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_36 (Dropout)           (None, 500, 1)       0           ['multi_head_attention_17[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_33 (TFOpL  (None, 500, 1)      0           ['dropout_36[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_32[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_35 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_33[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_32 (Conv1D)             (None, 500, 4)       8           ['layer_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 500, 4)       0           ['conv1d_32[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_33 (Conv1D)             (None, 500, 1)       5           ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_34 (TFOpL  (None, 500, 1)      0           ['conv1d_33[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_33[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_36 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_34[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_18 (Multi  (None, 500, 1)      7169        ['layer_normalization_36[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 500, 1)       0           ['multi_head_attention_18[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_35 (TFOpL  (None, 500, 1)      0           ['dropout_38[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_34[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_37 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_35[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_34 (Conv1D)             (None, 500, 4)       8           ['layer_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_39 (Dropout)           (None, 500, 4)       0           ['conv1d_34[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_35 (Conv1D)             (None, 500, 1)       5           ['dropout_39[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_36 (TFOpL  (None, 500, 1)      0           ['conv1d_35[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_35[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_38 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_36[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_19 (Multi  (None, 500, 1)      7169        ['layer_normalization_38[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_40 (Dropout)           (None, 500, 1)       0           ['multi_head_attention_19[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_37 (TFOpL  (None, 500, 1)      0           ['dropout_40[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_36[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_39 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_37[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_36 (Conv1D)             (None, 500, 4)       8           ['layer_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)           (None, 500, 4)       0           ['conv1d_36[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_37 (Conv1D)             (None, 500, 1)       5           ['dropout_41[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_38 (TFOpL  (None, 500, 1)      0           ['conv1d_37[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_37[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_3 (Gl  (None, 500)         0           ['tf.__operators__.add_38[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128)          64128       ['global_average_pooling1d_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)           (None, 128)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 2)            258         ['dropout_42[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 93,130\n",
            "Trainable params: 93,130\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "id": "Zin3w41vognL",
        "outputId": "9b1f518d-8502-42c3-f356-61750bb4315a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/transformer_h4_history', \"rb\") as file_pi:\n",
        "    history_4h = pickle.load(file_pi)\n",
        "   "
      ],
      "metadata": {
        "id": "DfYi3lZTWIHu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "50a13d43-bfb2-4764-a03f-84a5256a3604"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-da7ea55838c1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/transformer_h4_history'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mhistory_4h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/transformer_h4_history'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pylab as plt\n",
        "print accuracy and loss\n",
        "\n",
        "plot the history .... what do you see?"
      ],
      "metadata": {
        "id": "u6Ibqh6wENkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "Official version: In about 110-120 epochs (25s each on Colab), the model reaches a training\n",
        "accuracy of ~0.95, validation accuracy of ~84 and a testing\n",
        "accuracy of ~85, without hyperparameter tuning. And that is for a model\n",
        "with less than 100k parameters. Of course, parameter count and accuracy could be\n",
        "improved by a hyperparameter search and a more sophisticated learning rate\n",
        "schedule, or a different optimizer.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/timeseries_transformer_classification) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/timeseries_transformer_classification).\n",
        "\n",
        "**Reality** these transformer blocks are not doing anything! \n",
        "- there is no positional encoding\n",
        "- the time series are 1D and its not clear that the performance holds going from multivariate (tockenized) to univariate\n"
      ],
      "metadata": {
        "id": "HV47sNxGognL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "\"\"\"\n",
        "create another model with :\n",
        "1 multiattention head size 256, \n",
        "0 transformer blocks\n",
        "4 neurons in the convolutional layers\n",
        "128 neurons in the feed forward layers\n",
        "dropout 40% on the transformer layers\n",
        "dropout 25% on the feed forward layers\n",
        "\n",
        "compile it with a sparse_categorical_crossentropy,\n",
        "choose the kearas adam optimizer with a learning rate of 1e-4\n",
        "monitor the sparse_categorical_accuracy metric\n",
        "\"\"\"\n",
        "\n",
        "model.summary()\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "g6ReU0K01Mxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "print accuracy and loss\n",
        "\n",
        "plot the history .... what do you see?"
      ],
      "metadata": {
        "id": "BcJd-Ql-1jAY"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}